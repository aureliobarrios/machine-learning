{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making A Simple Neural Network In PyTorch then comparing it to a neural network that will be made from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 5x5 square convolution\n",
    "        # kernel\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)  # 5*5 from image dimension\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, input):\n",
    "        # Convolution layer C1: 1 input image channel, 6 output channels,\n",
    "        # 5x5 square convolution, it uses RELU activation function, and\n",
    "        # outputs a Tensor with size (N, 6, 28, 28), where N is the size of the batch\n",
    "        c1 = F.relu(self.conv1(input))\n",
    "        # Subsampling layer S2: 2x2 grid, purely functional,\n",
    "        # this layer does not have any parameter, and outputs a (N, 6, 14, 14) Tensor\n",
    "        s2 = F.max_pool2d(c1, (2, 2))\n",
    "        # Convolution layer C3: 6 input channels, 16 output channels,\n",
    "        # 5x5 square convolution, it uses RELU activation function, and\n",
    "        # outputs a (N, 16, 10, 10) Tensor\n",
    "        c3 = F.relu(self.conv2(s2))\n",
    "        # Subsampling layer S4: 2x2 grid, purely functional,\n",
    "        # this layer does not have any parameter, and outputs a (N, 16, 5, 5) Tensor\n",
    "        s4 = F.max_pool2d(c3, 2)\n",
    "        # Flatten operation: purely functional, outputs a (N, 400) Tensor\n",
    "        s4 = torch.flatten(s4, 1)\n",
    "        # Fully connected layer F5: (N, 400) Tensor input,\n",
    "        # and outputs a (N, 120) Tensor, it uses RELU activation function\n",
    "        f5 = F.relu(self.fc1(s4))\n",
    "        # Fully connected layer F6: (N, 120) Tensor input,\n",
    "        # and outputs a (N, 84) Tensor, it uses RELU activation function\n",
    "        f6 = F.relu(self.fc2(f5))\n",
    "        # Gaussian layer OUTPUT: (N, 84) Tensor input, and\n",
    "        # outputs a (N, 10) Tensor\n",
    "        output = self.fc3(f6)\n",
    "        return output\n",
    "\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network From Scratch\n",
    "\n",
    "Here we will build the fundamentals of Neural Networks from scratch, starting with the perceptron.\n",
    "\n",
    "### Perceptrons\n",
    "\n",
    "The perceptron approximates a single neuron with n binary inputs. The perceptron computes a weighted sum of its inputs and \"fires\" if that weighted sum is zero or greater."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the perceptron step function\n",
    "def step_function(x):\n",
    "    return 1 if x >= 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build dot product function that will be used\n",
    "def dot(v, w):\n",
    "    return sum(v_i * w_i for v_i, w_i in zip(v, w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perceptron_output(weights, bias, x):\n",
    "    \"\"\"\n",
    "    returns 1 if the perceptron 'fires' and 0 if not\n",
    "    \"\"\"\n",
    "    calculation = dot(weights, x) + bias\n",
    "    return step_function(calculation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed-Forward Neural Networks\n",
    "\n",
    "There are obvious limitations to a simple perceptron so in order to perform more complicated solutions we incorporate what are called \"feed-forward neural networks\". These networks consist of discrete layers of neurons, each connected to the next and where each layer feeds into the next layer.\n",
    "\n",
    "These networks are incorporated using the following layers\n",
    "- input layer: receives inputs and feeds them forward unchanged\n",
    "- hidden layer (could be multiple): consists of neurons that take the outputs of the previous layer, performs some calculations and passes the result to the next layer\n",
    "- output layer: produces the final outputs\n",
    "\n",
    "Feed-Forward networks also implement a smooth approximation of the step function within perceptrons, this smooth approximation is known as the sigmoid function. This is because in order to train these networks we must implement some calculus and therefore we need continuous functions. The sigmoid function can also be thought of as the logistic function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "#sigmoid function\n",
    "def sigmoid(t):\n",
    "    return 1 / (1 + math.exp(-t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore in order to calculate the output of our neurons, we use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neuron_output(weights, inputs):\n",
    "    return sigmoid(dot(weights, inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore we can think of a neuron and represent it as a list of weights whose length is one more than the number of inputs to that neuron (to account for the bias term).\n",
    "\n",
    "Then we can represent a neural \"network\" as a list of layers, where each layer is just a list of the neurons in that layer.\n",
    "\n",
    "Putting it all together our neural network will be a list (layers) of lists (neurons) of lists (weights)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_forward(neural_network, input_vector):\n",
    "    \"\"\"\n",
    "    takes in a list of neural network (represented as a list\n",
    "    of lists of lists of weights) and returns the output \n",
    "    from forward-propagating the input\n",
    "    \"\"\"\n",
    "    #save the outputs\n",
    "    outputs = []\n",
    "    #process one layer at a time\n",
    "    for layer in neural_network:\n",
    "        #add a bias to the input\n",
    "        input_with_bias = input_vector + [1]\n",
    "        #compute the output for each neuron in layer\n",
    "        output = [neuron_output(neuron, input_with_bias) for neuron in layer]\n",
    "        #store the ouptut for each neuron\n",
    "        outputs.append(output)\n",
    "        #then the output to the next layer is the output of this one\n",
    "        input_vector = output\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing out the network by building an XOR gate (or but not and) which could not be done by using a single perceptron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build the xor network\n",
    "xor_network = [\n",
    "    #hidden layer\n",
    "    [[20, 20, -30],\n",
    "     [20, 20, -10]],\n",
    "     #output layer\n",
    "     [[-60, 60, -30]]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 [9.38314668300676e-14]\n",
      "0 1 [0.9999999999999059]\n",
      "1 0 [0.9999999999999059]\n",
      "1 1 [9.383146683006828e-14]\n"
     ]
    }
   ],
   "source": [
    "for x in [0, 1]:\n",
    "    for y in [0, 1]:\n",
    "        #feed_forward produces the outputs of every neuron\n",
    "        #feed_forward[-1] is the outputs of the output-layer neurons\n",
    "        print(x, y, feed_forward(xor_network, [x, y])[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, since we scaled the weights up, the neuron outputs are either really close to 0 or really close to 1. By using a hidden layer, we are able to feed the output of an \"and\" neuron and the output of an \"or\" neuron into a \"second input but not first input\" neuron. This is what makes the XOR gate.\n",
    "\n",
    "### Backpropagation\n",
    "\n",
    "In order to build neural networks we must train them on data. One popular approach for training which is very similar to gradient descent is called **backpropagation**.\n",
    "\n",
    "In order to perform backpropagation we must have: a training set that consists of input vectors and corresponding target output vectors, and our network must have some set of weights that are adjusted following this algorithm...\n",
    "1. Run `feed_forward` on an input vector to produce the outputs of all the neurons in the network\n",
    "\n",
    "2. This results in an error for each output neuron (the difference between its output and its target)\n",
    "\n",
    "3. Compute the gradient of this error as a function of the neurons weights and adjust the weights in the direction that most decreases the error\n",
    "\n",
    "4. \"Propagate\" these output errors backward to infer errors for the hidden layer\n",
    "\n",
    "5. Compute the gradients of these errors and adjust the hidden layer's weights in the same manner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def propagate(network, input_vector, targets):\n",
    "    #get the outputs\n",
    "    hidden_outputs, outputs = feed_forward(network, input_vector)\n",
    "    #the output * (1 - output) is from the derivative of sigmoid\n",
    "    output_deltas = [output * (1 - output) * (output - target)\n",
    "                     for output, target in zip(outputs, targets)]\n",
    "    #adjust the weights for output layer, one neuron at a time\n",
    "    for i, output_neuron in enumerate(network[-1]):\n",
    "        #focus on the ith output layer neuron\n",
    "        for j, hidden_output in enumerate(hidden_outputs + [1]):\n",
    "            #adjust the jth weight based on both this neurons delta and its jth input\n",
    "            output_neuron[j] -= output_deltas[i] * hidden_output\n",
    "    #back propagate errors to hidden layer\n",
    "    hidden_deltas = [hidden_output * (1 - hidden_output) * \n",
    "                     dot(output_deltas, [n[i] for n in network[-1]])\n",
    "                     for i, hidden_output in enumerate(hidden_outputs)]\n",
    "    #adjust the weights for hidden layer, one neuron at a time\n",
    "    for i, hidden_neuron in enumerate(network[0]):\n",
    "        for j, input in enumerate(input_vector + [1]):\n",
    "            hidden_neuron[j] -= hidden_deltas[i] * input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to note that this is essentially the very similar to writing out the squared error as a function of the weights and used stochastic gradient descent to minimize the error. This is a fundamental concept in machine learning.\n",
    "\n",
    "### Neural Network Example\n",
    "\n",
    "For the following example we will be using our neural network to defeat a simple CAPTCHA, where numbers are represented on a 5x5 grid where every i,j value in the grid is a 0 (meaning this pixel is white in the image) or 1 (meaning this pixel is black in the image).\n",
    "\n",
    "i.e. The number 0 will be represented by \n",
    "\n",
    "[\n",
    "1, 1, 1, 1, 1\\\n",
    "1, 0, 0, 0, 1\\\n",
    "1, 0, 0, 0, 1\\\n",
    "1, 0, 0, 0, 1\\\n",
    "1, 1, 1, 1, 1\n",
    "]\n",
    "\n",
    "We will display our targets from 0 to 9 to be the corresponding index in a list i.e. the correct output for 4 will be: `[0, 0, 0, 0, 1, 0, 0, 0, 0, 0]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build out our targets\n",
    "targets = [[1 if i == j else 0 for i in range(10)] for j in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
