{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformers From Scratch\n",
    "\n",
    "Notebook where I build a Transformer from scratch using PyTorch neural networks.\n",
    "\n",
    "### Read In Input File\n",
    "\n",
    "First we are going to read in our input file which is a collection of all the text of Shakespeare. We can see the total vocabolary denoted in the `chars` variable. This a set of all the possible unique characters that have been found within Shakespeare's works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in shakespear input file\n",
    "with open('../notebooks/data/input.txt', 'r', encoding = 'utf-8') as file:\n",
    "    text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "#get all the unique characters that occur in the text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we must create an initial mapping, in order to map each unique character to a unique integer. We do this so that we can eventually train our model with the text but obviously representing each unique character as an integer which can then be processed by our tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a mapping of characters to integers\n",
    "string_to_integer = {ch: i for i, ch in enumerate(chars)}\n",
    "#create a mapping of integers to characters\n",
    "integer_to_string = {i: ch for i, ch in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoding our initial text into a PyTorch tensor using our simple encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build simple encoder that takes a string and outputs a list of integers\n",
    "encoder = lambda input_string: [string_to_integer[character] for character in input_string]\n",
    "#build a simple decoder that takes a list of integers, and outputs a string\n",
    "decoder = lambda input_list: ''.join([integer_to_string[integer] for integer in input_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "#encode entire text dataset and store it into a torch.Tensor\n",
    "data = torch.tensor(encoder(text), dtype=torch.long)\n",
    "print(data[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get first 90% characters\n",
    "n = int(0.9 * len(data))\n",
    "#splitting our dataset into training and validation sets for future use\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#make batches for training\n",
    "block_size = 8\n",
    "train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a visualizer on how the training is going to occur within the tensors. We want to be able to predict the next character based on a sequence/context that is provided. Therefore when we consider a block size of 8, that means we have a sequence of 8 characters, but rather than just train our model on this sequence we will recursively train on each possible context behind the sequence of characters as visualized below. This will provide more context and informaton on how sequences affect your next probable target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([18]) the target is: 47\n",
      "when input is tensor([18, 47]) the target is: 56\n",
      "when input is tensor([18, 47, 56]) the target is: 57\n",
      "when input is tensor([18, 47, 56, 57]) the target is: 58\n",
      "when input is tensor([18, 47, 56, 57, 58]) the target is: 1\n",
      "when input is tensor([18, 47, 56, 57, 58,  1]) the target is: 15\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15]) the target is: 47\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target is: 58\n"
     ]
    }
   ],
   "source": [
    "#training visualizer\n",
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "\n",
    "#see training inputs\n",
    "for trial in range(block_size):\n",
    "    context = x[:trial+1]\n",
    "    target = y[trial]\n",
    "    print(f\"when input is {context} the target is: {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have visualized how batching will work we will create some batch dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seeding for replication\n",
    "torch.manual_seed(1337)\n",
    "#the number of batches used for training\n",
    "batch_size = 4\n",
    "#the maximum context length for predictions\n",
    "block_size = 8\n",
    "\n",
    "#function that generates batches\n",
    "def get_batch(split):\n",
    "    #differentiate between training and validation\n",
    "    if split == \"train\":\n",
    "        data = train_data\n",
    "    else:\n",
    "        data = val_data\n",
    "    #getting random indices between data for sampling of batches\n",
    "    random_index = torch.randint(len(data) - block_size, (batch_size, ))\n",
    "    #sampling batches and stacking them to same dimension\n",
    "    x = torch.stack([data[index:index+block_size] for index in random_index])\n",
    "    #sampling batches for target variable\n",
    "    y = torch.stack([data[index+1:index+block_size+1] for index in random_index])\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we could get training batches, we will test out training on a simple BiGram Language Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "#embeddings visualized\n",
    "token_embedding_table = nn.Embedding(vocab_size, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(65, 65)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_embedding_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "#creating a simple BiGram Model Class\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    #constructor for this class\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        #build an embedding for our vocab characters\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "    \n",
    "    def forward(self, idx, targets):\n",
    "        #build logits, idx and targets are both (B, T) tensor of integers\n",
    "        logits = self.token_embedding_table(idx) # (B, T, C)\n",
    "\n",
    "        #re-organize our logits to fit cross entropy function\n",
    "        B, T, C = logits.shape\n",
    "        logits = logits.view(B * T, C)\n",
    "        targets = targets.view(B * T)\n",
    "\n",
    "        #build our loss function\n",
    "        loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        #idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            #get the predictions\n",
    "            logits, loss = self(idx)\n",
    "            #focus only on the last time step\n",
    "            logits = logits[:, -1, :] #becomes (B, C)\n",
    "            #apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            #sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            #append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "tensor(5.0364, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#get current batch\n",
    "xb, yb = get_batch('train')\n",
    "#build our bigram model\n",
    "bigram = BigramLanguageModel(vocab_size)\n",
    "#run our model\n",
    "logits, loss = bigram(xb, yb)\n",
    "\n",
    "print(logits.shape)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
